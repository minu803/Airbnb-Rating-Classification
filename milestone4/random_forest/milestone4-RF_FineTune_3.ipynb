{"cells": [{"cell_type": "markdown", "id": "89b8d881-4bd6-44a7-9424-3bd5a8aaa706", "metadata": {}, "source": "#### Random Forest Model Fine-Tuning"}, {"cell_type": "code", "execution_count": 2, "id": "b3dd8b5a-53b6-43c6-a20f-e65056824eaf", "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql.functions import col, isnan, count, when, isnull, size, split\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, FloatType, DateType\nfrom pyspark.sql.functions import col, regexp_replace\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier, RandomForestClassificationModel\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nimport matplotlib.pyplot as plt\nimport pandas as pd"}, {"cell_type": "code", "execution_count": 3, "id": "484a9a02-bed9-43c9-a440-dd1a93f3237c", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/04/13 07:22:44 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n24/04/13 07:22:44 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n24/04/13 07:22:44 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n24/04/13 07:22:44 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "spark = SparkSession.builder.appName('fine_tune_random_forest_attempt3').getOrCreate()"}, {"cell_type": "code", "execution_count": 4, "id": "f516d566-f396-49dc-bf8f-b4e939db56a9", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "trainingDataPath = \"gs://ds5460-tlee-spring2024/notebooks/jupyter/data/usa/combined_datasets/trainingDataTransformed.parquet\"\ntestDataPath = \"gs://ds5460-tlee-spring2024/notebooks/jupyter/data/usa/combined_datasets/dataWithZeroReviewTransformed.parquet\"\n\n\n# Load the transformed data\nreviews_data = spark.read.parquet(trainingDataPath)\nzero_review_data = spark.read.parquet(testDataPath)"}, {"cell_type": "code", "execution_count": 5, "id": "8db611e4-9304-4301-87d5-3b09b8a302d8", "metadata": {}, "outputs": [], "source": "# Do 70/30 split on seed = 42 for consistency\ntrain_data,test_data = reviews_data.randomSplit([0.7,0.3], seed=42)"}, {"cell_type": "code", "execution_count": 5, "id": "461b6954-0ce5-4250-8bc6-1ca3fc3b68ad", "metadata": {}, "outputs": [], "source": "# Initialize a new RandomForestClassifier\nrfc = RandomForestClassifier(labelCol='target_label', featuresCol='features', maxBins=2200)"}, {"cell_type": "code", "execution_count": 6, "id": "a53ce409-29ef-4e5a-87d1-a3f5186790dd", "metadata": {}, "outputs": [], "source": "# Define the parameter grid by testing different fine-tune parameters\nparamGrid = (ParamGridBuilder()\n             .addGrid(rfc.numTrees, [20])\n             .addGrid(rfc.maxDepth, [12])\n             .build())\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"target_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n\n# Setup the CrossValidator\ncv = CrossValidator(estimator=rfc,\n                    estimatorParamMaps=paramGrid,\n                    evaluator=evaluator,\n                    numFolds=3)"}, {"cell_type": "code", "execution_count": 7, "id": "7491c26c-9f1c-4195-9b91-0d0c1fc166dd", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/04/13 06:35:40 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1482.0 KiB\n24/04/13 06:35:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n24/04/13 06:35:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n24/04/13 06:36:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 6.2 MiB\n24/04/13 06:36:17 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1121.2 KiB\n24/04/13 06:36:21 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n24/04/13 06:37:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1484.8 KiB\n24/04/13 06:37:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n24/04/13 06:37:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n24/04/13 06:37:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 6.3 MiB\n24/04/13 06:37:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1118.4 KiB\n24/04/13 06:38:00 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n24/04/13 06:38:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1503.0 KiB\n24/04/13 06:39:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n24/04/13 06:39:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n24/04/13 06:39:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 6.4 MiB\n24/04/13 06:39:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1149.1 KiB\n24/04/13 06:39:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n24/04/13 06:41:03 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1513.8 KiB\n24/04/13 06:41:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n24/04/13 06:41:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n24/04/13 06:41:43 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 6.9 MiB\n24/04/13 06:41:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1341.3 KiB\n                                                                                \r"}], "source": "# Fit the CrossValidator to the training data\ncv_model = cv.fit(train_data)"}, {"cell_type": "code", "execution_count": 8, "id": "ca1e68ff-5505-4f5b-b91d-c34900881a9b", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/04/13 06:42:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n[Stage 120:============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "Best Model Accuracy: 60.01%\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Use the best model to make predictions on the test data\nbest_model = cv_model.bestModel\npredictions = best_model.transform(test_data)\n\n# Evaluate the best model's accuracy on the test data\naccuracy = evaluator.evaluate(predictions)\nprint('Best Model Accuracy: {:.2f}%'.format(accuracy * 100))"}, {"cell_type": "code", "execution_count": 9, "id": "66c924e2-f134-4221-b5ee-77556e24622f", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/04/13 06:43:27 WARN org.apache.spark.scheduler.TaskSetManager: Stage 124 contains a task of very large size (2476 KiB). The maximum recommended task size is 1000 KiB.\n                                                                                \r"}], "source": "# Specify model save path to use it for the load model notebook\nmodel_path = \"gs://ds5460-tlee-spring2024/notebooks/jupyter//data/usa/combined_datasets/rfc_attempt_3\"\n\n# Save the model\nbest_model.save(model_path)"}, {"cell_type": "code", "execution_count": 6, "id": "3e888863-1aeb-4f4c-bab7-e5fa79dbd5e4", "metadata": {}, "outputs": [], "source": "spark.stop()"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}