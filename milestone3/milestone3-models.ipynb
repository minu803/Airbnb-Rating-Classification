{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3: Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col, isnan, count, when, isnull, size, split\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, FloatType, DateType\n",
    "from pyspark.sql.functions import col, regexp_replace\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('final_project').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put in your bucket and folder path without the csv file\n",
    "BUCKET = 'gs://bdbucket27/notebooks/jupyter/' \n",
    "\n",
    "data_with_review = spark.read.csv(BUCKET + 'target_column_with_review.csv',inferSchema=True,header=True)\n",
    "data_with_zero_review = spark.read.csv(BUCKET + 'target_column_with_zero_review.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id=772438920837360569, host_id=382318476, host_since='2020-12-30', host_location='Unknown', host_is_superhost='t', host_listings_count=1.0, host_total_listings_count=3.0, host_has_profile_pic='t', host_identity_verified='t', neighborhood='Southwest Ranches', latitude=26.0338992, longitude=-80.3346054, room_type='Entire home/apt', accommodates=8, num_bath=3.0, bedrooms=4.0, beds=6.0, price=500.0, number_of_reviews=2, review_scores_value=5.0, calculated_host_listings_count=1, city='Broward County', amenities_count=14, neighborhood_city='Southwest Ranches Broward County', full_time_host='f', host_verifications_clean='ep', essential_amenities=3, target='Exceptional')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check the data was read in properly\n",
    "data_with_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id=827736378366911479, host_id=475630606, host_since='2022-08-18', host_location='Unknown', host_is_superhost='f', host_listings_count=1.0, host_total_listings_count=3.0, host_has_profile_pic='t', host_identity_verified='t', neighborhood='Fort Lauderdale', latitude=26.09393643124416, longitude=-80.13759087771177, room_type='Entire home/apt', accommodates=2, num_bath=1.0, bedrooms=1.0, beds=1.0, price=222.0, number_of_reviews=0, review_scores_value=None, calculated_host_listings_count=1, city='Broward County', amenities_count=10, neighborhood_city='Fort Lauderdale Broward County', full_time_host='f', host_verifications_clean='p', essential_amenities=3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check the data was read in properly\n",
    "data_with_zero_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that can't be used in the feature pipeline as not applicable to zero reviews\n",
    "data_with_review = data_with_review.drop('review_scores_value')\n",
    "data_with_review = data_with_review.drop('number_of_reviews')\n",
    "data_with_zero_review = data_with_zero_review.drop('review_scores_value')\n",
    "data_with_zero_review = data_with_zero_review.drop('number_of_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'host_id',\n",
       " 'host_since',\n",
       " 'host_location',\n",
       " 'host_is_superhost',\n",
       " 'host_listings_count',\n",
       " 'host_total_listings_count',\n",
       " 'host_has_profile_pic',\n",
       " 'host_identity_verified',\n",
       " 'neighborhood',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'num_bath',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'price',\n",
       " 'calculated_host_listings_count',\n",
       " 'city',\n",
       " 'amenities_count',\n",
       " 'neighborhood_city',\n",
       " 'full_time_host',\n",
       " 'host_verifications_clean',\n",
       " 'essential_amenities',\n",
       " 'target']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check the columns in data_with_review\n",
    "data_with_review.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of string features to be indexed and the features that are already numeric\n",
    "stringFeatures = ['host_location', 'host_is_superhost', 'host_has_profile_pic',\n",
    "                  'host_identity_verified', 'city', 'room_type', 'full_time_host', \n",
    "                  'host_verifications_clean']\n",
    "\n",
    "numericFeatures = ['host_listings_count', 'host_total_listings_count', 'accommodates', \n",
    "                   'num_bath','bedrooms', 'beds', 'price', 'calculated_host_listings_count',\n",
    "                   'amenities_count', 'essential_amenities']\n",
    "\n",
    "# Create StringIndexer stages for the stringFeatures - call the numeric version as _indexed\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_indexed\", stringOrderType=\"alphabetAsc\").setHandleInvalid(\"skip\")\n",
    "            for column in stringFeatures]\n",
    "\n",
    "# Create a StringIndexer for the target column and naming it as target_label after converting it to numeric\n",
    "labelIndexer = StringIndexer(inputCol='target', outputCol='target_label').setHandleInvalid(\"skip\")\n",
    "\n",
    "# Create VectorAssembler stage for the features\n",
    "assemblerInputs = [column+\"_indexed\" for column in stringFeatures] + numericFeatures\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "# Create and run the pipeline with the target indexer added to pipeline\n",
    "pipeline = Pipeline(stages=indexers + [assembler, labelIndexer])\n",
    "\n",
    "# Fit the pipeline on the training data - which is all of data that has reviews\n",
    "pipelineModel = pipeline.fit(data_with_review)\n",
    "\n",
    "# Alternative code if we wanted to do the train/test split earlier as it depends if we want to do 80/20 or 70/30.\n",
    "# trainingDataTransformed = pipelineModel.transform(trainingData)\n",
    "# testDataTransformed = pipelineModel.transform(testData)\n",
    "\n",
    "# Apply the pipeline on the data_with_review - trainingDataTransformed seen below will have to go through a train/test split\n",
    "trainingDataTransformed = pipelineModel.transform(data_with_review)\n",
    "\n",
    "\n",
    "# To apply the pipeline on the data_with_zero_review, which is missing the target, we need to do the following:\n",
    "# Extracting the transformation stages from the fitted pipeline model\n",
    "# Excldue the LabelIndexer from final pipeline stage as we are applying on the prediction dataset\n",
    "transformationStages = pipelineModel.stages[:-1]\n",
    "\n",
    "# Manually apply each transformation stage to the data_with_zero_review\n",
    "transformedData = data_with_zero_review\n",
    "for stage in transformationStages:\n",
    "    transformedData = stage.transform(transformedData)\n",
    "\n",
    "# We now have dataWithZeroReviewTransformed which doesn't have the target_label column to see what is our final model predictions\n",
    "dataWithZeroReviewTransformed = transformedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if Feature Pipeline was done properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- host_id: integer (nullable = true)\n",
      " |-- host_since: string (nullable = true)\n",
      " |-- host_location: string (nullable = true)\n",
      " |-- host_is_superhost: string (nullable = true)\n",
      " |-- host_listings_count: double (nullable = true)\n",
      " |-- host_total_listings_count: double (nullable = true)\n",
      " |-- host_has_profile_pic: string (nullable = true)\n",
      " |-- host_identity_verified: string (nullable = true)\n",
      " |-- neighborhood: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: integer (nullable = true)\n",
      " |-- num_bath: double (nullable = true)\n",
      " |-- bedrooms: double (nullable = true)\n",
      " |-- beds: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- calculated_host_listings_count: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- amenities_count: integer (nullable = true)\n",
      " |-- neighborhood_city: string (nullable = true)\n",
      " |-- full_time_host: string (nullable = true)\n",
      " |-- host_verifications_clean: string (nullable = true)\n",
      " |-- essential_amenities: integer (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      " |-- host_location_indexed: double (nullable = false)\n",
      " |-- host_is_superhost_indexed: double (nullable = false)\n",
      " |-- host_has_profile_pic_indexed: double (nullable = false)\n",
      " |-- host_identity_verified_indexed: double (nullable = false)\n",
      " |-- city_indexed: double (nullable = false)\n",
      " |-- room_type_indexed: double (nullable = false)\n",
      " |-- full_time_host_indexed: double (nullable = false)\n",
      " |-- host_verifications_clean_indexed: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- target_label: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingDataTransformed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- host_id: integer (nullable = true)\n",
      " |-- host_since: string (nullable = true)\n",
      " |-- host_location: string (nullable = true)\n",
      " |-- host_is_superhost: string (nullable = true)\n",
      " |-- host_listings_count: double (nullable = true)\n",
      " |-- host_total_listings_count: double (nullable = true)\n",
      " |-- host_has_profile_pic: string (nullable = true)\n",
      " |-- host_identity_verified: string (nullable = true)\n",
      " |-- neighborhood: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- accommodates: integer (nullable = true)\n",
      " |-- num_bath: double (nullable = true)\n",
      " |-- bedrooms: double (nullable = true)\n",
      " |-- beds: double (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- calculated_host_listings_count: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- amenities_count: integer (nullable = true)\n",
      " |-- neighborhood_city: string (nullable = true)\n",
      " |-- full_time_host: string (nullable = true)\n",
      " |-- host_verifications_clean: string (nullable = true)\n",
      " |-- essential_amenities: integer (nullable = true)\n",
      " |-- host_location_indexed: double (nullable = false)\n",
      " |-- host_is_superhost_indexed: double (nullable = false)\n",
      " |-- host_has_profile_pic_indexed: double (nullable = false)\n",
      " |-- host_identity_verified_indexed: double (nullable = false)\n",
      " |-- city_indexed: double (nullable = false)\n",
      " |-- room_type_indexed: double (nullable = false)\n",
      " |-- full_time_host_indexed: double (nullable = false)\n",
      " |-- host_verifications_clean_indexed: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataWithZeroReviewTransformed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+----------+-------------+-----------------+-------------------+-------------------------+--------------------+----------------------+-----------------+----------+-----------+---------------+------------+--------+--------+----+-----+------------------------------+--------------+---------------+--------------------------------+--------------+------------------------+-------------------+-----------+---------------------+-------------------------+----------------------------+------------------------------+------------+-----------------+----------------------+--------------------------------+-------------------------------------------------------------------------------+------------+\n",
      "|id                |host_id  |host_since|host_location|host_is_superhost|host_listings_count|host_total_listings_count|host_has_profile_pic|host_identity_verified|neighborhood     |latitude  |longitude  |room_type      |accommodates|num_bath|bedrooms|beds|price|calculated_host_listings_count|city          |amenities_count|neighborhood_city               |full_time_host|host_verifications_clean|essential_amenities|target     |host_location_indexed|host_is_superhost_indexed|host_has_profile_pic_indexed|host_identity_verified_indexed|city_indexed|room_type_indexed|full_time_host_indexed|host_verifications_clean_indexed|features                                                                       |target_label|\n",
      "+------------------+---------+----------+-------------+-----------------+-------------------+-------------------------+--------------------+----------------------+-----------------+----------+-----------+---------------+------------+--------+--------+----+-----+------------------------------+--------------+---------------+--------------------------------+--------------+------------------------+-------------------+-----------+---------------------+-------------------------+----------------------------+------------------------------+------------+-----------------+----------------------+--------------------------------+-------------------------------------------------------------------------------+------------+\n",
      "|772438920837360569|382318476|2020-12-30|Unknown      |t                |1.0                |3.0                      |t                   |t                     |Southwest Ranches|26.0338992|-80.3346054|Entire home/apt|8           |3.0     |4.0     |6.0 |500.0|1                             |Broward County|14             |Southwest Ranches Broward County|f             |ep                      |3                  |Exceptional|2227.0               |1.0                      |1.0                         |1.0                           |1.0         |0.0              |0.0                   |1.0                             |[2227.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,3.0,8.0,3.0,4.0,6.0,500.0,1.0,14.0,3.0]|2.0         |\n",
      "+------------------+---------+----------+-------------+-----------------+-------------------+-------------------------+--------------------+----------------------+-----------------+----------+-----------+---------------+------------+--------+--------+----+-----+------------------------------+--------------+---------------+--------------------------------+--------------+------------------------+-------------------+-----------+---------------------+-------------------------+----------------------------+------------------------------+------------+-----------------+----------------------+--------------------------------+-------------------------------------------------------------------------------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingDataTransformed.show(n=1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|       host_location|host_location_indexed|\n",
      "+--------------------+---------------------+\n",
      "|             Unknown|               2227.0|\n",
      "|  Fort Lauderdale FL|                725.0|\n",
      "|Buenos Aires Arge...|                293.0|\n",
      "|           Irvine CA|                973.0|\n",
      "|            Miami FL|               1351.0|\n",
      "+--------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------+-----------------+\n",
      "|      room_type|room_type_indexed|\n",
      "+---------------+-----------------+\n",
      "|Entire home/apt|              0.0|\n",
      "|Entire home/apt|              0.0|\n",
      "|Entire home/apt|              0.0|\n",
      "|Entire home/apt|              0.0|\n",
      "|Entire home/apt|              0.0|\n",
      "|Entire home/apt|              0.0|\n",
      "|   Private room|              2.0|\n",
      "|   Private room|              2.0|\n",
      "|   Private room|              2.0|\n",
      "|   Private room|              2.0|\n",
      "+---------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Doing a quick check that the string indexer worked and converted the categorical values into an index\n",
    "trainingDataTransformed.select('host_location', 'host_location_indexed').show(5)\n",
    "\n",
    "trainingDataTransformed.select('room_type', 'room_type_indexed').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|     target|target_label|\n",
      "+-----------+------------+\n",
      "|Exceptional|         2.0|\n",
      "|   Mediocre|         1.0|\n",
      "|   Mediocre|         1.0|\n",
      "|   Mediocre|         1.0|\n",
      "|       Good|         0.0|\n",
      "+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking that the target_label got created successfully\n",
    "trainingDataTransformed.select('target', 'target_label').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------+------------+\n",
      "|features                                                                           |target_label|\n",
      "+-----------------------------------------------------------------------------------+------------+\n",
      "|[2227.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,3.0,8.0,3.0,4.0,6.0,500.0,1.0,14.0,3.0]    |2.0         |\n",
      "|[725.0,0.0,1.0,0.0,1.0,0.0,0.0,8.0,1.0,12.0,6.0,2.0,2.0,4.0,186.0,3.0,22.0,4.0]    |1.0         |\n",
      "|[293.0,0.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,20.0,7.0,2.0,2.0,5.0,297.0,6.0,17.0,3.0]    |1.0         |\n",
      "|[973.0,0.0,1.0,0.0,1.0,0.0,0.0,1.0,1.0,5.0,4.0,1.0,1.0,2.0,162.0,5.0,69.0,5.0]     |1.0         |\n",
      "|[1351.0,0.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,17.0,2.0,1.0,1.0,1.0,92.0,15.0,17.0,4.0]   |0.0         |\n",
      "|[1941.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,1930.0,6.0,2.0,2.0,4.0,258.0,23.0,38.0,5.0]|0.0         |\n",
      "|[1351.0,1.0,1.0,1.0,1.0,2.0,0.0,4.0,1.0,1.0,4.0,1.0,1.0,2.0,100.0,1.0,72.0,4.0]    |0.0         |\n",
      "|[542.0,0.0,1.0,1.0,1.0,2.0,0.0,8.0,1.0,19.0,4.0,1.0,1.0,2.0,189.0,16.0,60.0,5.0]   |1.0         |\n",
      "|[1351.0,1.0,1.0,0.0,1.0,2.0,0.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,63.0,2.0,67.0,5.0]     |1.0         |\n",
      "|[1351.0,0.0,1.0,1.0,1.0,2.0,0.0,1.0,1.0,13.0,2.0,1.0,1.0,1.0,127.0,6.0,51.0,5.0]   |1.0         |\n",
      "|[39.0,0.0,1.0,1.0,1.0,2.0,0.0,1.0,1.0,112.0,4.0,2.0,1.0,1.0,300.0,5.0,81.0,5.0]    |2.0         |\n",
      "|[1351.0,0.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,14.0,6.0,2.0,2.0,2.0,218.0,8.0,16.0,4.0]   |0.0         |\n",
      "|[711.0,0.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,2.0,4.0,1.0,2.0,3.0,155.0,1.0,52.0,5.0]     |0.0         |\n",
      "|[1719.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,32.0,9.0,3.5,4.0,6.0,1764.0,4.0,57.0,5.0]  |2.0         |\n",
      "|[2125.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,10.0,2.0,1.0,1.0,1.0,91.0,4.0,52.0,5.0]    |0.0         |\n",
      "|[1351.0,1.0,1.0,0.0,1.0,2.0,0.0,1.0,1.0,2.0,3.0,1.0,1.0,1.0,60.0,2.0,66.0,5.0]     |1.0         |\n",
      "|[2125.0,0.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,292.0,6.0,1.0,2.0,3.0,313.0,28.0,24.0,4.0] |3.0         |\n",
      "|[435.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,72.0,2.0,1.0,1.0,1.0,101.0,70.0,54.0,5.0]   |0.0         |\n",
      "|[725.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,12.0,4.0,2.0,2.0,2.0,236.0,10.0,63.0,5.0]   |0.0         |\n",
      "|[725.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,23.0,6.0,3.0,3.0,3.0,440.0,19.0,52.0,5.0]   |0.0         |\n",
      "+-----------------------------------------------------------------------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examine all of the features with a particular target_label\n",
    "trainingDataTransformed.select(\"features\", \"target_label\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[2227.0,0.0,1.0,1...|\n",
      "|[39.0,0.0,1.0,1.0...|\n",
      "|[223.0,0.0,1.0,1....|\n",
      "|[1719.0,0.0,1.0,0...|\n",
      "|[1237.0,1.0,1.0,1...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examine if the dataWithZeroReviewTransformed features look like it was done correctly\n",
    "dataWithZeroReviewTransformed.select('features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For trainingDataTransformed:\n",
      "Number of Rows: 143995, Number of Columns: 36\n",
      "For dataWithZeroReviewTransformed:\n",
      "Number of Rows: 40408, Number of Columns: 34\n"
     ]
    }
   ],
   "source": [
    "# See the count of each dataset to get an understanding on sizes:\n",
    "num_rows = trainingDataTransformed.count()\n",
    "num_columns = len(trainingDataTransformed.first())\n",
    "\n",
    "print(\"For trainingDataTransformed:\")\n",
    "print(f\"Number of Rows: {num_rows}, Number of Columns: {num_columns}\")\n",
    "\n",
    "num_rows = dataWithZeroReviewTransformed.count()\n",
    "num_columns = len(dataWithZeroReviewTransformed.first())\n",
    "\n",
    "print(\"For dataWithZeroReviewTransformed:\")\n",
    "print(f\"Number of Rows: {num_rows}, Number of Columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the train test split - we need to discuss what are we settng for the threshold\n",
    "train_data,test_data = trainingDataTransformed.randomSplit([0.7,0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree model \n",
    "dtc = DecisionTreeClassifier(labelCol='target_label',featuresCol='features',maxBins = 2432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit to the train_data\n",
    "dtc_model = dtc.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions for the test_data, which is the data with the ground truth known\n",
    "dtc_predictions = dtc_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"target_label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_acc = acc_evaluator.evaluate(dtc_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Decision Tree had an accuracy of: 50.90%\n"
     ]
    }
   ],
   "source": [
    "print('A Decision Tree had an accuracy of: {0:2.2f}%'.format(dtc_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+------------+-----------------------------+--------------------------------------------------------------------------------+----------+\n",
      "|features                                                                        |target_label|rawPrediction                |probability                                                                     |prediction|\n",
      "+--------------------------------------------------------------------------------+------------+-----------------------------+--------------------------------------------------------------------------------+----------+\n",
      "|[1509.0,0.0,1.0,1.0,10.0,0.0,0.0,4.0,7.0,9.0,1.0,1.0,1.0,1.0,150.0,3.0,32.0,5.0]|1.0         |[4574.0,7121.0,2810.0,1787.0]|[0.2807512889761846,0.4370856862263688,0.17247728946722318,0.10968573533022342] |1.0       |\n",
      "|[1237.0,1.0,1.0,1.0,7.0,2.0,0.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,118.0,2.0,66.0,5.0] |0.0         |[6977.0,1082.0,1984.0,87.0]  |[0.688746298124383,0.10681145113524186,0.19585389930898323,0.008588351431391906]|0.0       |\n",
      "+--------------------------------------------------------------------------------+------------+-----------------------------+--------------------------------------------------------------------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Just seeing what is inside the rfc_predictions\n",
    "dtc_predictions.select(\"features\", \"target_label\", \"rawPrediction\", \"probability\", \"prediction\").show(truncate=False, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.509004\n",
      "+------------+-----+----+----+---+\n",
      "|target_label|    0|   1|   2|  3|\n",
      "+------------+-----+----+----+---+\n",
      "|         0.0|14291|2815|1331| 48|\n",
      "|         1.0| 4499|4930|1500|179|\n",
      "|         2.0| 5572|2251|2560|155|\n",
      "|         3.0|  591|1437| 808|182|\n",
      "+------------+-----+----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(dtc_predictions)\n",
    "print(\"Test Accuracy = %g\" % (accuracy))\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Count predictions\n",
    "confusion_matrix = dtc_predictions.groupBy('target_label').pivot('prediction', [0,1,2,3]).count().na.fill(0).orderBy('target_label')\n",
    "confusion_matrix.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set Accuracy = 0.5138705416116248\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DecisionTreeClassificationModel' object has no attribute 'getMaxDepth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-d2dfa572e3e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Print best model's parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mbestModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best Param (maxDepth): {bestModel.getMaxDepth()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best Param (maxBins): {bestModel.getMaxBins()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best Param (minInstancesPerNode): {bestModel.getMinInstancesPerNode()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DecisionTreeClassificationModel' object has no attribute 'getMaxDepth'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "\n",
    "# Initialize the DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(labelCol='target_label', featuresCol='features', maxBins=2432)\n",
    "\n",
    "\n",
    "# Create the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(dtc.maxDepth, [5, 10, 20]) \\\n",
    "    .addGrid(dtc.maxBins, [2432]) \\\n",
    "    .addGrid(dtc.minInstancesPerNode, [1, 2, 4]) \\\n",
    "    .build()\n",
    "\n",
    "# Create the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Create the CrossValidator\n",
    "# cv = CrossValidator(estimator=dtc,\n",
    "#                     estimatorParamMaps=paramGrid,\n",
    "#                     evaluator=evaluator,\n",
    "#                     numFolds=3,\n",
    "#                     parallelism=4)  # Use 3+ folds in practice\n",
    "\n",
    "cv = TrainValidationSplit(estimator=dtc,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator,\n",
    "                    trainRatio=0.8,\n",
    "                    parallelism=4)  \n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = cv.fit(train_data)\n",
    "\n",
    "# Make predictions on test data. cvModel uses the best model found.\n",
    "prediction = cvModel.transform(test_data)\n",
    "\n",
    "# Evaluate the best model's performance\n",
    "#f1_score = evaluator.evaluate(prediction)\n",
    "acc = acc_evaluator.evaluate(prediction)\n",
    "print(\"Test set Accuracy = \" + str(acc))\n",
    "\n",
    "# Print best model's parameters\n",
    "bestModel = cvModel.bestModel\n",
    "print(f\"Best Param (maxDepth): {bestModel.getMaxDepth()}\")\n",
    "print(f\"Best Param (maxBins): {bestModel.getMaxBins()}\")\n",
    "print(f\"Best Param (minInstancesPerNode): {bestModel.getMinInstancesPerNode()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model's parameters:\n",
      " - maxDepth: 10\n",
      " - maxBins: 2432\n",
      " - minInstancesPerNode: 1\n"
     ]
    }
   ],
   "source": [
    "# Get the best model from CrossValidator\n",
    "bestModel = cvModel.bestModel\n",
    "\n",
    "# Print the parameters of the best model\n",
    "print(\"Best model's parameters:\")\n",
    "print(f\" - maxDepth: {bestModel._java_obj.getMaxDepth()}\")\n",
    "print(f\" - maxBins: {bestModel._java_obj.getMaxBins()}\")\n",
    "print(f\" - minInstancesPerNode: {bestModel._java_obj.getMinInstancesPerNode()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5138705416116248\n",
      "+------------+-----+----+----+---+\n",
      "|target_label|    0|   1|   2|  3|\n",
      "+------------+-----+----+----+---+\n",
      "|         0.0|13841|2787|1845| 12|\n",
      "|         1.0| 4317|5052|1675| 64|\n",
      "|         2.0| 5092|2197|3131|118|\n",
      "|         3.0|  575|1399| 895|149|\n",
      "+------------+-----+----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the best model found by CrossValidator to make predictions on the test data\n",
    "predictions = cvModel.transform(test_data)\n",
    "\n",
    "# Evaluate the model's accuracy on the test data\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Compute the accuracy on the test data\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "confusion_matrix = predictions.groupBy('target_label').pivot('prediction', [0, 1, 2, 3]).count().na.fill(0).orderBy('target_label')\n",
    "confusion_matrix.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Model on 0 Review Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the trained Random Forest model to the prediction dataset - the hold out zero reviews data\n",
    "zeroReviewPredictions = cvModel.transform(dataWithZeroReviewTransformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+-------------------------+--------------------------------------------------------------------------------+----------+\n",
      "|features                                                                          |rawPrediction            |probability                                                                     |prediction|\n",
      "+----------------------------------------------------------------------------------+-------------------------+--------------------------------------------------------------------------------+----------+\n",
      "|[2227.0,0.0,1.0,1.0,1.0,0.0,0.0,7.0,1.0,3.0,2.0,1.0,1.0,1.0,222.0,1.0,10.0,3.0]   |[277.0,479.0,751.0,262.0]|[0.1565856416054268,0.2707744488411532,0.4245336348219333,0.14810627473148671]  |2.0       |\n",
      "|[39.0,0.0,1.0,1.0,1.0,2.0,0.0,1.0,1.0,112.0,2.0,2.0,2.0,4.0,500.0,5.0,29.0,3.0]   |[104.0,181.0,94.0,53.0]  |[0.24074074074074073,0.41898148148148145,0.2175925925925926,0.12268518518518519]|1.0       |\n",
      "|[223.0,0.0,1.0,1.0,1.0,0.0,0.0,4.0,1.0,6.0,16.0,4.0,6.0,12.0,868.0,4.0,68.0,5.0]  |[1.0,12.0,0.0,0.0]       |[0.07692307692307693,0.9230769230769231,0.0,0.0]                                |1.0       |\n",
      "|[1719.0,0.0,1.0,0.0,1.0,2.0,0.0,1.0,1.0,3.0,2.0,1.0,1.0,2.0,47.0,1.0,14.0,3.0]    |[15.0,12.0,32.0,9.0]     |[0.22058823529411764,0.17647058823529413,0.47058823529411764,0.1323529411764706]|2.0       |\n",
      "|[1237.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,465.0,5.0,1.0,2.0,4.0,629.0,11.0,24.0,4.0]|[2.0,8.0,36.0,5.0]       |[0.0392156862745098,0.1568627450980392,0.7058823529411765,0.09803921568627451]  |2.0       |\n",
      "+----------------------------------------------------------------------------------+-------------------------+--------------------------------------------------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying predictions, probabilities, and features\n",
    "zeroReviewPredictions.select(\"features\", \"rawPrediction\", \"probability\", \"prediction\").show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the train test split - we need to discuss what are we settng for the threshold\n",
    "train_data,test_data = trainingDataTransformed.randomSplit([0.7,0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(labelCol='target_label',featuresCol='features', maxBins = 2432)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit to the train_data\n",
    "rfc_model = rfc.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions for the test_data, which is the data with the ground truth known\n",
    "rfc_predictions = rfc_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"target_label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_acc = acc_evaluator.evaluate(rfc_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random forest ensemble had an accuracy of: 50.81%\n"
     ]
    }
   ],
   "source": [
    "print('A random forest ensemble had an accuracy of: {0:2.2f}%'.format(rfc_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.5081\n",
      "+------------+-----+----+----+---+\n",
      "|target_label|    0|   1|   2|  3|\n",
      "+------------+-----+----+----+---+\n",
      "|         0.0|15381|2115| 986|  3|\n",
      "|         1.0| 5636|4285|1162| 25|\n",
      "|         2.0| 6473|1835|2197| 33|\n",
      "|         3.0|  826|1359| 772| 61|\n",
      "+------------+-----+----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(rfc_predictions)\n",
    "print(\"Test Accuracy = %g\" % (accuracy))\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Count predictions\n",
    "confusion_matrix = rfc_predictions.groupBy('target_label').pivot('prediction', [0,1,2,3]).count().na.fill(0).orderBy('target_label')\n",
    "confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rfc = RandomForestClassifier(labelCol='target_label', featuresCol='features', maxBins=2432)\n",
    "\n",
    "# Create the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rfc.maxDepth, [5, 10, 20]) \\\n",
    "    .addGrid(rfc.maxBins, [2432]) \\\n",
    "    .addGrid(rfc.minInstancesPerNode, [1, 2, 4]) \\\n",
    "    .addGrid(rfc.numTrees, [10, 20, 30]) \\\n",
    "    .build()  \n",
    "\n",
    "\n",
    "# Assuming evaluator is already defined as MulticlassClassificationEvaluator with accuracy as the metric\n",
    "\n",
    "# Use TrainValidationSplit for hyperparameter tuning\n",
    "tvs = TrainValidationSplit(estimator=rfc,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=evaluator,\n",
    "                           trainRatio=0.8,  # 80% for training, 20% for validation\n",
    "                           parallelism=4)  # Adjust based on your available resources\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "tvsModel = tvs.fit(train_data)\n",
    "\n",
    "# Make predictions on test data. tvsModel uses the best model found.\n",
    "predictions = tvsModel.transform(test_data)\n",
    "\n",
    "# Evaluate the best model's performance\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set Accuracy = \" + str(accuracy))\n",
    "\n",
    "# Print best model's parameters\n",
    "bestModel = tvsModel.bestModel\n",
    "print(f\"Best Param (maxDepth): {bestModel.getMaxDepth()}\")\n",
    "print(f\"Best Param (maxBins): {bestModel.getMaxBins()}\")\n",
    "print(f\"Best Param (minInstancesPerNode): {bestModel.getMinInstancesPerNode()}\")\n",
    "print(f\"Best Param (numTrees): {bestModel.getNumTrees()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best model found by CrossValidator to make predictions on the test data\n",
    "predictions = tvsModel.transform(test_data)\n",
    "\n",
    "# Evaluate the model's accuracy on the test data\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"target_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Compute the accuracy on the test data\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "confusion_matrix = predictions.groupBy('target_label').pivot('prediction', [0, 1, 2, 3]).count().na.fill(0).orderBy('target_label')\n",
    "confusion_matrix.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Model on 0 Review Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+------------+-----------------------------------------------------------------------------+---------------------------------------------------------------------------------+----------+\n",
      "|features                                                                        |target_label|rawPrediction                                                                |probability                                                                      |prediction|\n",
      "+--------------------------------------------------------------------------------+------------+-----------------------------------------------------------------------------+---------------------------------------------------------------------------------+----------+\n",
      "|[1509.0,0.0,1.0,1.0,10.0,0.0,0.0,4.0,7.0,9.0,1.0,1.0,1.0,1.0,150.0,3.0,32.0,5.0]|1.0         |[6.838360705466454,7.29565682035771,4.060407784157887,1.8055746900179512]    |[0.34191803527332265,0.36478284101788544,0.2030203892078943,0.09027873450089755] |1.0       |\n",
      "|[1237.0,1.0,1.0,1.0,7.0,2.0,0.0,1.0,2.0,3.0,1.0,1.0,1.0,1.0,118.0,2.0,66.0,5.0] |0.0         |[12.887417387217319,2.6367924520091623,4.168560089523278,0.30723007125023966]|[0.6443708693608661,0.13183962260045815,0.20842800447616394,0.015361503562511985]|0.0       |\n",
      "+--------------------------------------------------------------------------------+------------+-----------------------------------------------------------------------------+---------------------------------------------------------------------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Just seeing what is inside the rfc_predictions\n",
    "rfc_predictions.select(\"features\", \"target_label\", \"rawPrediction\", \"probability\", \"prediction\").show(truncate=False, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the trained Random Forest model to the prediction dataset - the hold out zero reviews data\n",
    "zeroReviewPredictions = rfc_model.transform(dataWithZeroReviewTransformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+---------------------------------------------------------------------------+---------------------------------------------------------------------------------+----------+\n",
      "|features                                                                          |rawPrediction                                                              |probability                                                                      |prediction|\n",
      "+----------------------------------------------------------------------------------+---------------------------------------------------------------------------+---------------------------------------------------------------------------------+----------+\n",
      "|[2227.0,0.0,1.0,1.0,1.0,0.0,0.0,7.0,1.0,3.0,2.0,1.0,1.0,1.0,222.0,1.0,10.0,3.0]   |[4.717063937420779,4.928950811128885,8.068563945734876,2.2854213057154587] |[0.23585319687103895,0.24644754055644427,0.40342819728674384,0.11427106528577294]|2.0       |\n",
      "|[39.0,0.0,1.0,1.0,1.0,2.0,0.0,1.0,1.0,112.0,2.0,2.0,2.0,4.0,500.0,5.0,29.0,3.0]   |[5.525568483607355,8.161259922896548,3.9207584563051383,2.3924131371909616]|[0.27627842418036774,0.4080629961448274,0.19603792281525692,0.11962065685954808] |1.0       |\n",
      "|[223.0,0.0,1.0,1.0,1.0,0.0,0.0,4.0,1.0,6.0,16.0,4.0,6.0,12.0,868.0,4.0,68.0,5.0]  |[7.471512174755624,6.606861210178554,4.301366464464823,1.6202601506009988] |[0.3735756087377812,0.3303430605089277,0.21506832322324115,0.08101300753004995]  |0.0       |\n",
      "|[1719.0,0.0,1.0,0.0,1.0,2.0,0.0,1.0,1.0,3.0,2.0,1.0,1.0,2.0,47.0,1.0,14.0,3.0]    |[4.777434053845889,4.778246684809403,8.161002741889344,2.283316519455364]  |[0.2388717026922944,0.23891233424047012,0.40805013709446714,0.11416582597276818] |2.0       |\n",
      "|[1237.0,1.0,1.0,1.0,1.0,0.0,0.0,1.0,1.0,465.0,5.0,1.0,2.0,4.0,629.0,11.0,24.0,4.0]|[8.398834043134453,4.683542590757535,5.529156146440015,1.3884672196679981] |[0.4199417021567226,0.23417712953787673,0.2764578073220007,0.06942336098339989]  |0.0       |\n",
      "+----------------------------------------------------------------------------------+---------------------------------------------------------------------------+---------------------------------------------------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying predictions, probabilities, and features\n",
    "zeroReviewPredictions.select(\"features\", \"rawPrediction\", \"probability\", \"prediction\").show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(labelCol=\"target_label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GBTClassifier is designed for binary classification tasks, which means it expects the target column (labelCol) to have exactly two unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbt_model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
